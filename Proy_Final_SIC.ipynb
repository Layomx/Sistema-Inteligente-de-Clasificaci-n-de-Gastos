{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1z0sr7_VhD8kEwhaF3WJwpv2x2ISPlPyE","authorship_tag":"ABX9TyOcl8UCFFVBqpkyrGsECXGW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["PARTE 1: RED NEURONAL\n"],"metadata":{"id":"TGv6uflovrld"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZetY_MOIsT_F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764857841106,"user_tz":300,"elapsed":26615,"user":{"displayName":"Elbin Hernández","userId":"03404993536554449402"}},"outputId":"d4a09485-657b-4582-b2e3-d96a155a85d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- INICIANDO ENTRENAMIENTO ---\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6255 - loss: 0.8712 - val_accuracy: 0.7703 - val_loss: 0.4862\n","Epoch 2/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8319 - loss: 0.4232 - val_accuracy: 0.9611 - val_loss: 0.2503\n","Epoch 3/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9633 - loss: 0.2220 - val_accuracy: 0.9823 - val_loss: 0.1481\n","Epoch 4/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9768 - loss: 0.1383 - val_accuracy: 0.9894 - val_loss: 0.0997\n","Epoch 5/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.1069 - val_accuracy: 0.9965 - val_loss: 0.0791\n","Epoch 6/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9858 - loss: 0.0839 - val_accuracy: 1.0000 - val_loss: 0.0675\n","Epoch 7/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0701 - val_accuracy: 0.9894 - val_loss: 0.0600\n","Epoch 8/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9878 - loss: 0.0623 - val_accuracy: 0.9965 - val_loss: 0.0544\n","Epoch 9/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0610 - val_accuracy: 0.9823 - val_loss: 0.0526\n","Epoch 10/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9912 - loss: 0.0534 - val_accuracy: 0.9929 - val_loss: 0.0449\n","Epoch 11/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9848 - loss: 0.0520 - val_accuracy: 0.9788 - val_loss: 0.0493\n","Epoch 12/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0409 - val_accuracy: 0.9894 - val_loss: 0.0429\n","Epoch 13/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0431 - val_accuracy: 0.9894 - val_loss: 0.0393\n","Epoch 14/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9919 - loss: 0.0397 - val_accuracy: 0.9894 - val_loss: 0.0372\n","Epoch 15/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9864 - loss: 0.0459 - val_accuracy: 0.9894 - val_loss: 0.0375\n","Epoch 16/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0310 - val_accuracy: 0.9894 - val_loss: 0.0370\n","Epoch 17/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9879 - loss: 0.0371 - val_accuracy: 0.9823 - val_loss: 0.0391\n","Epoch 18/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9917 - loss: 0.0299 - val_accuracy: 1.0000 - val_loss: 0.0303\n","Epoch 19/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0299 - val_accuracy: 0.9965 - val_loss: 0.0283\n","Epoch 20/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9968 - loss: 0.0229 - val_accuracy: 1.0000 - val_loss: 0.0290\n","Epoch 21/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9945 - loss: 0.0323 - val_accuracy: 0.9894 - val_loss: 0.0307\n","Epoch 22/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0274 - val_accuracy: 0.9929 - val_loss: 0.0296\n","Epoch 23/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.0312 - val_accuracy: 0.9929 - val_loss: 0.0289\n","Epoch 24/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0320 - val_accuracy: 0.9894 - val_loss: 0.0299\n","Epoch 25/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9896 - loss: 0.0295 - val_accuracy: 1.0000 - val_loss: 0.0252\n","Epoch 26/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0245 - val_accuracy: 0.9894 - val_loss: 0.0274\n","Epoch 27/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0301 - val_accuracy: 0.9929 - val_loss: 0.0261\n","Epoch 28/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9922 - loss: 0.0241 - val_accuracy: 0.9894 - val_loss: 0.0272\n","Epoch 29/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9891 - loss: 0.0235 - val_accuracy: 0.9965 - val_loss: 0.0216\n","Epoch 30/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0232 - val_accuracy: 0.9929 - val_loss: 0.0276\n","Epoch 31/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9954 - loss: 0.0223 - val_accuracy: 0.9965 - val_loss: 0.0205\n","Epoch 32/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9966 - loss: 0.0199 - val_accuracy: 1.0000 - val_loss: 0.0206\n","Epoch 33/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 0.0276 - val_accuracy: 1.0000 - val_loss: 0.0204\n","Epoch 34/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9934 - loss: 0.0261 - val_accuracy: 0.9823 - val_loss: 0.0325\n","Epoch 35/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9949 - loss: 0.0219 - val_accuracy: 0.9929 - val_loss: 0.0240\n","Epoch 36/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0208 - val_accuracy: 0.9965 - val_loss: 0.0210\n","Epoch 37/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9967 - loss: 0.0222 - val_accuracy: 0.9965 - val_loss: 0.0194\n","Epoch 38/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9881 - loss: 0.0278 - val_accuracy: 0.9894 - val_loss: 0.0305\n","Epoch 39/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9896 - loss: 0.0274 - val_accuracy: 1.0000 - val_loss: 0.0186\n","Epoch 40/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0240 - val_accuracy: 0.9929 - val_loss: 0.0196\n","Epoch 41/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9946 - loss: 0.0240 - val_accuracy: 0.9894 - val_loss: 0.0241\n","Epoch 42/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.0189 - val_accuracy: 0.9929 - val_loss: 0.0203\n","Epoch 43/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9972 - loss: 0.0199 - val_accuracy: 0.9859 - val_loss: 0.0262\n","Epoch 44/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9883 - loss: 0.0271 - val_accuracy: 0.9823 - val_loss: 0.0286\n","Epoch 45/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0238 - val_accuracy: 0.9965 - val_loss: 0.0189\n","Epoch 46/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9935 - loss: 0.0175 - val_accuracy: 0.9929 - val_loss: 0.0211\n","Epoch 47/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9858 - loss: 0.0246 - val_accuracy: 0.9929 - val_loss: 0.0216\n","Epoch 48/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9883 - loss: 0.0217 - val_accuracy: 0.9894 - val_loss: 0.0199\n","Epoch 49/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.0167 - val_accuracy: 0.9965 - val_loss: 0.0175\n","Epoch 50/50\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.0213 - val_accuracy: 0.9929 - val_loss: 0.0260\n","\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9919 - loss: 0.0237 \n","\n","PRECISIÓN FINAL DEL MODELO: 99.29%\n","\n"," todo bien. Se han creado 'modelo_facturas.keras' y 'scaler.pkl'.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","import os\n","\n","# uploading el dataset\n","if not os.path.exists('/content/drive/MyDrive/Proyecto_Final_SIC/dataset_listo_para_entrenar.csv'):\n","    print(\"Error: dataset no enceontrado\")\n","    exit()\n","\n","df = pd.read_csv('/content/drive/MyDrive/Proyecto_Final_SIC/dataset_listo_para_entrenar.csv')\n","\n","# Definimos Entrada (X) y Salida (y)\n","# X = El precio limpio\n","# y = La categoría (0, 1, 2)\n","X = df[['Total_Limpio']].values\n","y = df['Categoria'].values\n","\n","# Preprocesamiento: hay qyue jugar con numeros grandes\n","# Las redes neuronales funcionan mal con números grandes (como 1000, 5000).\n","# El Scaler convierte esos números a una escala pequeña (ej: 0.5, -1.2) para que la red aprenda rápido.\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# aplico data spliting: 80% para entrenar, 20% para examinar qué tan bien aprendió\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","#Arquitectura de la red neuronal\n","model = tf.keras.models.Sequential([\n","    # Capa de entrada y oculta 1: 64 neuronas\n","    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n","\n","    # Capa oculta 2: 32 neuronas\n","    tf.keras.layers.Dense(32, activation='relu'),\n","\n","    # Capa de SALIDA: 3 neuronas (porque tenemos 3 categorías: Bajo, Medio, Alto)\n","    # Softmax convierte los resultados en probabilidades\n","    tf.keras.layers.Dense(3, activation='softmax')\n","])\n","\n","# Compilacionn\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy', # Usamos esta loss function porque las etiquetas son enteros (0,1,2)\n","              metrics=['accuracy'])\n","\n","#training\n","print(\"--- INICIANDO ENTRENAMIENTO ---\")\n","# Epochs = Cuántas veces repasará los datos (50 veces)\n","historia = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=1, validation_data=(X_test, y_test))\n","\n","#metricas\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"\\nPRECISIÓN FINAL DEL MODELO: {accuracy * 100:.2f}%\")\n","\n","# 6. Guardamos los archivos necesarios pa la aplicación final\n","# Guardamos el modelo\n","model.save('modelo_facturas.keras')\n","# Guardamos el escalador\n","with open('scaler.pkl', 'wb') as f:\n","    pickle.dump(scaler, f)\n","\n","print(\"\\n todo bien. Se han creado 'modelo_facturas.keras' y 'scaler.pkl'.\")"]},{"cell_type":"markdown","source":["PARTE 2: PROBANDO EL MODELO CON IMAGENES O PDFS"],"metadata":{"id":"ST6PIakgvv9Z"}},{"cell_type":"code","source":["#teseracto y popler para trabajar con pdfs\n","!sudo apt-get install tesseract-ocr\n","!sudo apt-get install libtesseract-dev\n","!sudo apt-get install poppler-utils\n","!pip install pytesseract pdf2image tensorflow scikit-learn pandas opencv-python-headless"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JVhwVzDPv5UX","executionInfo":{"status":"ok","timestamp":1765381724112,"user_tz":300,"elapsed":27092,"user":{"displayName":"Elbin Hernández","userId":"03404993536554449402"}},"outputId":"e33db0e9-2832-45ab-da5a-59fafe7c7da8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libarchive-dev libleptonica-dev\n","The following NEW packages will be installed:\n","  libarchive-dev libleptonica-dev libtesseract-dev\n","0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n","Need to get 3,743 kB of archives.\n","After this operation, 16.0 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.5 [581 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n","Fetched 3,743 kB in 2s (2,287 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libarchive-dev:amd64.\n","(Reading database ... 121713 files and directories currently installed.)\n","Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.5_amd64.deb ...\n","Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n","Selecting previously unselected package libleptonica-dev.\n","Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n","Unpacking libleptonica-dev (1.82.0-3build1) ...\n","Selecting previously unselected package libtesseract-dev:amd64.\n","Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n","Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n","Setting up libleptonica-dev (1.82.0-3build1) ...\n","Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n","Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n","Need to get 186 kB of archives.\n","After this operation, 697 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\n","Fetched 186 kB in 1s (240 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 121846 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\n","Unpacking poppler-utils (22.02.0-2ubuntu0.12) ...\n","Setting up poppler-utils (22.02.0-2ubuntu0.12) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Collecting pdf2image\n","  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n","Installing collected packages: pytesseract, pdf2image\n","Successfully installed pdf2image-1.17.0 pytesseract-0.3.13\n"]}]},{"cell_type":"markdown","source":["PARTE 3: CLASIFICANDO"],"metadata":{"id":"9kWtRXwRxecD"}},{"cell_type":"code","source":["import cv2\n","import pytesseract\n","import numpy as np\n","import tensorflow as tf\n","import pickle\n","import re\n","from pdf2image import convert_from_path\n","from google.colab import files\n","from PIL import Image\n","import io\n","\n","# Definimos las clases igual que en el entrenamiento\n","CLASES = {0: \"BAJO\", 1: \"MEDIO\", 2: \"ALTO\"}\n","\n","# Cargo el  modelo y escalador\n","try:\n","    model = tf.keras.models.load_model('/content/drive/MyDrive/Proyecto_Final_SIC/modelo_facturas.keras')\n","    with open('/content/drive/MyDrive/Proyecto_Final_SIC/scaler.pkl', 'rb') as f:\n","        scaler = pickle.load(f)\n","    print(\"Modelo y Scaler cargados en memoria.\")\n","except Exception as e:\n","    print(f\"Error cargando el modelo: {e}\")\n","\n","\n","# --- FUNCIONES DE LIMPIEZA Y OCR ---\n","\n","def limpiar_precio_complejo(texto):\n","    \"\"\"\n","    Busca el patrón 'Total $ XX,XX' y extrae el último valor numérico encontrado.\n","    \"\"\"\n","    if not isinstance(texto, str): return 0.0\n","    # Regex ajustado para encontrar números con decimales\n","    patron = r'(\\d+[\\.,]\\d+)'\n","    encontrados = re.findall(patron, texto)\n","\n","    if encontrados:\n","        # Tomamos el último valor encontrado (por si hay subtotales antes)\n","        precio_final = encontrados[-1]\n","        # Reemplazamos coma por punto y quitamos espacios\n","        precio_final = precio_final.replace(',', '.').replace(' ', '')\n","        try:\n","            return float(precio_final)\n","        except ValueError:\n","            return 0.0\n","    return 0.0\n","\n","def obtener_texto_de_archivo(nombre_archivo, contenido_bytes):\n","    \"\"\"\n","    Detecta si es Imagen o PDF y extrae el texto usando OCR.\n","    \"\"\"\n","    texto_completo = \"\"\n","\n","    # CASO 1: Es un PDF\n","    if nombre_archivo.lower().endswith('.pdf'):\n","        print(\" tipo de archivo: PDF. Convirtiendo a imagen para leer...\")\n","        try:\n","            # Convertimos el PDF (bytes) a lista de imágenes\n","            imagenes = convert_from_path(nombre_archivo)\n","\n","            # Procesamos solo la primera página\n","            texto_completo = pytesseract.image_to_string(imagenes[0])\n","            print(\"Lectura de PDF exitosa.\")\n","        except Exception as e:\n","            print(f\"Error leyendo PDF: {e}\")\n","\n","    # CASO 2: Es un .jpg o png\n","    else:\n","        print(\"imagen detectada. Leyendo...\")\n","        try:\n","            # Convertir bytes a imagen PIL\n","            image = Image.open(io.BytesIO(contenido_bytes))\n","            texto_completo = pytesseract.image_to_string(image)\n","            print(\" la imagen ha sido leida.\")\n","        except Exception as e:\n","            print(f\"Error leyendo Imagen: {e}\")\n","\n","    return texto_completo\n","\n","# --- PROGRAMA PRINCIPAL ---\n","\n","print(\"\\n sube una factura pa analizar\")\n","uploads = files.upload()\n","\n","for nombre_archivo, contenido in uploads.items():\n","    print(f\"\\n--- Analizando: {nombre_archivo} ---\")\n","\n","    # se obtiene texto (OCR)\n","    texto_extraido = obtener_texto_de_archivo(nombre_archivo, contenido)\n","\n","    # Buscamos especificamente la linea del Total\n","    # Usamos una búsqueda simple primero para ubicar la zona del 'Total'\n","    match = re.search(r'(Total.*)', texto_extraido, re.IGNORECASE | re.DOTALL)\n","\n","    precio_detectado = 0.0\n","\n","    if match:\n","        # Pasamos el fragmento de texto a nuestra función de limpieza\n","        linea_total = match.group(1)\n","        print(f\"Texto relevante encontrado: '{linea_total[:50]}...'\") # Mostramos un pedazo\n","        precio_detectado = limpiar_precio_complejo(linea_total)\n","    else:\n","        # Si no encuentra la palabra \"Total\", intenta buscar números al final del texto\n","        print(\"la palabra total no se ha encontrado. Intentando buscar números sueltos...\")\n","        precio_detectado = limpiar_precio_complejo(texto_extraido)\n","\n","    if precio_detectado > 0:\n","        print(f\" PRECIO IDENTIFICADO: ${precio_detectado}\")\n","\n","        # Se preprocesa para la IA (Escalar)\n","        # La red espera una lista de listas [[valor]]\n","        precio_array = np.array([[precio_detectado]])\n","        precio_escalado = scaler.transform(precio_array)\n","\n","        # fase de predicción\n","        prediccion = model.predict(precio_escalado, verbose=0)\n","        clase_id = np.argmax(prediccion) # El índice con mayor probabilidad\n","        confianza = np.max(prediccion) * 100\n","\n","        resultado = CLASES[clase_id]\n","\n","        print(\"\\n\" + \"=\"*40)\n","        print(f\"CLASIFICACIÓN IA: {resultado}\")\n","        print(f\"Confianza: {confianza:.2f}%\")\n","        print(\"=\"*40)\n","\n","    else:\n","        print(\"\\n error: No se pudo identificar un precio válido en el documento.\")\n","        print(\"Intenta subir una imagen con mejor calidad.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"id":"nHJ3cvvuxcIP","executionInfo":{"status":"ok","timestamp":1765381846178,"user_tz":300,"elapsed":118402,"user":{"displayName":"Elbin Hernández","userId":"03404993536554449402"}},"outputId":"ca914c7c-310c-478a-96d3-e79e05b8c73b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Modelo y Scaler cargados en memoria.\n","\n"," sube una factura pa analizar\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-d62c8d7e-d2cc-4f61-9fde-dc33d4d4f4b9\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d62c8d7e-d2cc-4f61-9fde-dc33d4d4f4b9\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving factura_rectificativa_2017_0001.png to factura_rectificativa_2017_0001.png\n","\n","--- Analizando: factura_rectificativa_2017_0001.png ---\n","imagen detectada. Leyendo...\n"," la imagen ha sido leida.\n","Texto relevante encontrado: 'Total Base Imponible: 80,00 €\n","\n","LV.A. 21%: -16,80 €...'\n"," PRECIO IDENTIFICADO: $84.8\n","\n","========================================\n","CLASIFICACIÓN IA: BAJO\n","Confianza: 100.00%\n","========================================\n"]}]}]}